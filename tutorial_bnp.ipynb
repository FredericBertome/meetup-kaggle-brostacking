{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KAGGLE MEETUP #10: BNP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 steps:\n",
    "    - 1: Load original dataset and create new variant\n",
    "    - 2: Train classifiers on each datasets and create oof predictions\n",
    "    - 3: Train classifiers on oof predictions from step 2\n",
    "    - 4: Average the test predictions of the step 3 classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "__author__ = 'Ardalan'\n",
    "import zipfile, copy, pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import ensemble, linear_model, svm\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "\n",
    "DATA_FOLDER = \"/home/ardalan/Documents/kaggle/bnp/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadFileinZipFile(zip_filename, dtypes=None, parsedate = None, password=None, **kvargs):\n",
    "    \"\"\"\n",
    "    Load zipfile to dataframe.\n",
    "    \"\"\"\n",
    "    with zipfile.ZipFile(zip_filename, 'r') as myzip:\n",
    "        if password:\n",
    "            myzip.setpassword(password)\n",
    "\n",
    "        inside_zip_filename = myzip.filelist[0].filename\n",
    "\n",
    "        if parsedate:\n",
    "            pd_data = pd.read_csv(myzip.open(inside_zip_filename), sep=',', parse_dates=parsedate, dtype=dtypes, **kvargs)\n",
    "        else:\n",
    "            pd_data = pd.read_csv(myzip.open(inside_zip_filename), sep=',', dtype=dtypes, **kvargs)\n",
    "        return pd_data, inside_zip_filename\n",
    "    \n",
    "def create_dataset1(pd_data):\n",
    "    \"\"\"\n",
    "    - Label Encoding Categorical variables\n",
    "    - Filling NaNs with -999\n",
    "    \"\"\"\n",
    "    \n",
    "    df = copy.copy(pd_data)\n",
    "    df['target'] = df['target'].fillna(-1)\n",
    "    df = df.fillna(-999)\n",
    "    df = df.drop('v107',1)\n",
    "\n",
    "    #Label encoding categorical variable\n",
    "    for col in df.select_dtypes(['object']):\n",
    "        df[col] = pd.factorize(df[col])[0]\n",
    "\n",
    "    #Extracting train and test data\n",
    "    pd_train = df[df.target >= 0]\n",
    "    pd_test = df[df.target == -1]\n",
    "\n",
    "    Y = pd_train['target'].values.astype(int)\n",
    "    test_idx = pd_test['ID'].values\n",
    "\n",
    "    X = np.array(pd_train.drop(['target','ID'],1))\n",
    "    X_test = np.array(pd_test.drop(['target','ID'],1))\n",
    "    \n",
    "    return X, Y, X_test, test_idx\n",
    "\n",
    "def create_dataset2(pd_data):\n",
    "    \"\"\"\n",
    "    - One Hot Encoding Categorical variables\n",
    "    - Filling NaNs with -999\n",
    "    \"\"\"\n",
    "\n",
    "    df = copy.copy(pd_data)\n",
    "    df['target'] = df['target'].fillna(-1)\n",
    "\n",
    "    df = df.drop('v107',1)\n",
    "\n",
    "    cat_vars_selected = ['v110', 'v112', 'v113', 'v125','v24', 'v3',\n",
    "                         'v30','v31', 'v47', 'v52', 'v56', 'v66', 'v71', 'v74',\n",
    "                         'v75', 'v79', 'v91']\n",
    "\n",
    "    df = pd.get_dummies(df, columns=cat_vars_selected, dummy_na=True)\n",
    "    df['v22'] = pd.factorize(df['v22'])[0]\n",
    "\n",
    "    df = df.fillna(-999)\n",
    "\n",
    "    #Extracting train and test data\n",
    "    pd_train = df[df.target >= 0]\n",
    "    pd_test = df[df.target == -1]\n",
    "\n",
    "    Y = pd_train['target'].values.astype(int)\n",
    "    test_idx = pd_test['ID'].values\n",
    "\n",
    "    X = np.array(pd_train.drop(['target','ID'],1))\n",
    "    X_test = np.array(pd_test.drop(['target','ID'],1))\n",
    "\n",
    "    return X, Y, X_test, test_idx\n",
    "\n",
    "\n",
    "def LoadParseBlendData(DATA_FOLDER):\n",
    "    import fnmatch, glob\n",
    "    folder = DATA_FOLDER + '*'\n",
    "    pattern = \"*.p\"\n",
    "    l_filenames = [path for path in glob.iglob(folder) if fnmatch.fnmatch(path, pattern)]\n",
    "    print(len(l_filenames), l_filenames)\n",
    "\n",
    "    dic_log = pickle.load(open(l_filenames[0], 'rb'))\n",
    "\n",
    "    test_idx = dic_log['test_idx']\n",
    "    Y = dic_log['blend_Y']\n",
    "\n",
    "    X = np.zeros((len(dic_log['blend_X']), len(l_filenames)))\n",
    "    X_test = np.zeros((len(dic_log['blend_X_test']), len(l_filenames)))\n",
    "\n",
    "    for i, filename in enumerate(l_filenames):\n",
    "        print(filename)\n",
    "\n",
    "        dic_log = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "        X[:, i] = dic_log['blend_X'][:, 0]\n",
    "        X_test[:, i] = dic_log['blend_X_test']\n",
    "    return X, Y, X_test, test_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load dataset and create variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Loading datasets\n",
    "pd_train, _ = loadFileinZipFile(DATA_FOLDER + \"train.csv.zip\")\n",
    "pd_test, _ = loadFileinZipFile(DATA_FOLDER + \"test.csv.zip\")\n",
    "pd_data = pd_train.append(pd_test)\n",
    "\n",
    "#Creating DATASET1 (D1)\n",
    "X, Y, X_test, test_idx = create_dataset1(pd_data)\n",
    "D1 = (X, Y, X_test, test_idx)\n",
    "\n",
    "#Creating DATASET2 (D2)\n",
    "X, Y, X_test, test_idx = create_dataset2(pd_data)\n",
    "D2 = (X, Y, X_test, test_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Train classifiers and create OOF predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def models():\n",
    "    \"\"\"\n",
    "    Create a list of [DATASET, Classifier] to train on\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    ET_params = {'n_estimators':50,'max_features': 50,'criterion': 'entropy',\n",
    "                 'min_samples_split': 4,'max_depth': 35, 'min_samples_leaf': 2}\n",
    "    \n",
    "    #ET_params = {'n_estimators':50}\n",
    "    \n",
    "    clfs = [\n",
    "        #[D2, linear_model.LogisticRegression(penalty='l2')]\n",
    "        [D1, ensemble.RandomForestClassifier(n_jobs=8, **ET_params)],\n",
    "        [D2, ensemble.RandomForestClassifier(n_jobs=8, **ET_params)],\n",
    "         \n",
    "        [D1, ensemble.ExtraTreesClassifier(n_jobs=8, **ET_params)],\n",
    "        [D2, ensemble.ExtraTreesClassifier(n_jobs=8, **ET_params)],\n",
    "    ]\n",
    "    return clfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier [0]\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
      "            max_depth=35, max_features=50, max_leaf_nodes=None,\n",
      "            min_samples_leaf=2, min_samples_split=4,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=8,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "Fold [0]\n",
      "train/val error: [0.1644|0.4761]\n",
      "Fold [1]\n",
      "train/val error: [0.1637|0.4818]\n",
      "Fold [2]\n",
      "train/val error: [0.1623|0.4803]\n",
      "Fold [3]\n",
      "train/val error: [0.1623|0.4919]\n",
      "Fold [4]\n",
      "train/val error: [0.1634|0.4897]\n",
      "Classifier [1]\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
      "            max_depth=35, max_features=50, max_leaf_nodes=None,\n",
      "            min_samples_leaf=2, min_samples_split=4,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=8,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "Fold [0]\n",
      "train/val error: [0.1958|0.4676]\n",
      "Fold [1]\n",
      "train/val error: [0.1960|0.4710]\n",
      "Fold [2]\n",
      "train/val error: [0.1938|0.4735]\n",
      "Fold [3]\n",
      "train/val error: [0.1952|0.4741]\n",
      "Fold [4]\n",
      "train/val error: [0.1934|0.4744]\n",
      "Classifier [2]\n",
      "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy',\n",
      "           max_depth=35, max_features=50, max_leaf_nodes=None,\n",
      "           min_samples_leaf=2, min_samples_split=4,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=8,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "Fold [0]\n",
      "train/val error: [0.0818|0.4650]\n",
      "Fold [1]\n",
      "train/val error: [0.0802|0.4675]\n",
      "Fold [2]\n",
      "train/val error: [0.0814|0.4713]\n",
      "Fold [3]\n",
      "train/val error: [0.0809|0.4707]\n",
      "Fold [4]\n",
      "train/val error: [0.0819|0.4729]\n",
      "Classifier [3]\n",
      "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy',\n",
      "           max_depth=35, max_features=50, max_leaf_nodes=None,\n",
      "           min_samples_leaf=2, min_samples_split=4,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=8,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "Fold [0]\n",
      "train/val error: [0.1963|0.4697]\n",
      "Fold [1]\n",
      "train/val error: [0.1962|0.4727]\n",
      "Fold [2]\n",
      "train/val error: [0.2023|0.4707]\n",
      "Fold [3]\n",
      "train/val error: [0.1981|0.4746]\n",
      "Fold [4]\n",
      "train/val error: [0.2003|0.4733]\n"
     ]
    }
   ],
   "source": [
    "clfs = models()\n",
    "skf = StratifiedKFold(Y, n_folds=5, shuffle=True , random_state=123)\n",
    "\n",
    "#Cross validation from a list of models\n",
    "for clf_indice, data_clf in enumerate(clfs):\n",
    "    \n",
    "    #Selecting a model from the list\n",
    "    print(\"Classifier [%i]\" % clf_indice)\n",
    "    \n",
    "    X = data_clf[0][0]\n",
    "    Y = data_clf[0][1]\n",
    "    X_test = data_clf[0][2]\n",
    "    test_idx = data_clf[0][3]\n",
    "    \n",
    "    clf = data_clf[1]\n",
    "    clf_name = clf.__class__.__name__\n",
    "    print(clf)\n",
    "    \n",
    "    blend_X = np.zeros((len(X), 1))\n",
    "    blend_Y = Y\n",
    "    blend_X_test = np.zeros((len(X_test), 1))\n",
    "    blend_X_test_fold = np.zeros((len(X_test), len(skf)))\n",
    "    \n",
    "    l_train_error = []\n",
    "    l_val_error = []\n",
    "    for fold_indice, (train_indices, val_indices) in enumerate(skf):\n",
    "        \n",
    "        print(\"Fold [%i]\" % fold_indice)\n",
    "        xtrain = X[train_indices]\n",
    "        ytrain = Y[train_indices]\n",
    "        xval = X[val_indices]\n",
    "        yval = Y[val_indices]\n",
    "        \n",
    "        clf.fit(xtrain, ytrain)\n",
    "        \n",
    "        ytrain_pred = clf.predict_proba(xtrain)[:,1]\n",
    "        yval_pred = clf.predict_proba(xval)[:,1]\n",
    "        ytest_pred = clf.predict_proba(X_test)[:,1]\n",
    "        \n",
    "        # filling blend data sets\n",
    "        blend_X[val_indices, 0] = yval_pred\n",
    "        blend_X_test_fold[:, fold_indice] = ytest_pred\n",
    "        \n",
    "        # evaluating model\n",
    "        train_error = log_loss(ytrain, ytrain_pred)\n",
    "        val_error = log_loss(yval, yval_pred)\n",
    "        l_train_error.append(train_error)\n",
    "        l_val_error.append(val_error)\n",
    "        print(\"train/val error: [{0:.4f}|{1:.4f}]\".format(train_error, val_error))\n",
    "        \n",
    "        \n",
    "    blend_X_test = np.mean(blend_X_test_fold, axis=1)\n",
    "    \n",
    "    diclogs = {'blend_X': blend_X,\n",
    "               'blend_Y': Y,\n",
    "               'blend_X_test': blend_X_test,\n",
    "               'test_idx': test_idx,\n",
    "               'clf_name': clf_name}\n",
    "        \n",
    "    #saving relevant information for blending later\n",
    "    filename = \"{}_tr-val_{:.4f}-{:.4f}\".format(clf_name, np.mean(l_train_error), np.mean(l_val_error))\n",
    "    pickle.dump(diclogs, open(DATA_FOLDER+filename + \".p\", 'wb'), protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    \n",
    "    #Saving final predictions\n",
    "    output_filename = DATA_FOLDER + filename + '.csv'\n",
    "    np.savetxt(output_filename, np.vstack((test_idx, blend_X_test)).T,\n",
    "               delimiter=',', fmt='%i,%.10f', header='ID,PredictedProb', comments=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Train classifiers on prediction from previous step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gathering prediction from previous layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 ['/home/ardalan/Documents/kaggle/bnp/data/LogisticRegression_tr-val_0.5202-0.5212.p', '/home/ardalan/Documents/kaggle/bnp/data/RandomForestClassifier_tr-val_0.1632-0.4840.p', '/home/ardalan/Documents/kaggle/bnp/data/RandomForestClassifier_tr-val_0.1948-0.4721.p', '/home/ardalan/Documents/kaggle/bnp/data/ExtraTreesClassifier_tr-val_0.0812-0.4695.p', '/home/ardalan/Documents/kaggle/bnp/data/ExtraTreesClassifier_tr-val_0.1986-0.4722.p']\n",
      "/home/ardalan/Documents/kaggle/bnp/data/LogisticRegression_tr-val_0.5202-0.5212.p\n",
      "/home/ardalan/Documents/kaggle/bnp/data/RandomForestClassifier_tr-val_0.1632-0.4840.p\n",
      "/home/ardalan/Documents/kaggle/bnp/data/RandomForestClassifier_tr-val_0.1948-0.4721.p\n",
      "/home/ardalan/Documents/kaggle/bnp/data/ExtraTreesClassifier_tr-val_0.0812-0.4695.p\n",
      "/home/ardalan/Documents/kaggle/bnp/data/ExtraTreesClassifier_tr-val_0.1986-0.4722.p\n"
     ]
    }
   ],
   "source": [
    "X, Y, X_test, test_idx = LoadParseBlendData(DATA_FOLDER)\n",
    "D_BLEND = (X, Y, X_test, test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def models():\n",
    "    \"\"\"\n",
    "    Create a list of [DATASET, Classifier] to train on\n",
    "    \"\"\"\n",
    "    clfs = [\n",
    "        [D_BLEND, ensemble.GradientBoostingClassifier(learning_rate=0.03, n_estimators=200,max_depth=5 ) ],  \n",
    "    ]\n",
    "    return clfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier [0]\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.03, loss='deviance',\n",
      "              max_depth=3, max_features=None, max_leaf_nodes=None,\n",
      "              min_samples_leaf=1, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=200,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "Fold [0]\n",
      "train/val error: [0.4538|0.4515]\n",
      "Fold [1]\n",
      "train/val error: [0.4527|0.4557]\n",
      "Fold [2]\n",
      "train/val error: [0.4528|0.4554]\n",
      "Fold [3]\n",
      "train/val error: [0.4518|0.4591]\n",
      "Fold [4]\n",
      "train/val error: [0.4524|0.4571]\n"
     ]
    }
   ],
   "source": [
    "clfs = models()\n",
    "skf = StratifiedKFold(Y, n_folds=5, shuffle=True , random_state=123)\n",
    "\n",
    "#Cross validation from a list of models\n",
    "for clf_indice, data_clf in enumerate(clfs):\n",
    "    \n",
    "    #Selecting a model from the list\n",
    "    print(\"Classifier [%i]\" % clf_indice)\n",
    "    \n",
    "    X = data_clf[0][0]\n",
    "    Y = data_clf[0][1]\n",
    "    X_test = data_clf[0][2]\n",
    "    test_idx = data_clf[0][3]\n",
    "    \n",
    "    clf = data_clf[1]\n",
    "    clf_name = clf.__class__.__name__\n",
    "    print(clf)\n",
    "    \n",
    "    blend_X = np.zeros((len(X), 1))\n",
    "    blend_Y = Y\n",
    "    blend_X_test = np.zeros((len(X_test), 1))\n",
    "    blend_X_test_fold = np.zeros((len(X_test), len(skf)))\n",
    "    \n",
    "    l_train_error = []\n",
    "    l_val_error = []\n",
    "    for fold_indice, (train_indices, val_indices) in enumerate(skf):\n",
    "        \n",
    "        print(\"Fold [%i]\" % fold_indice)\n",
    "        xtrain = X[train_indices]\n",
    "        ytrain = Y[train_indices]\n",
    "        xval = X[val_indices]\n",
    "        yval = Y[val_indices]\n",
    "        \n",
    "        clf.fit(xtrain, ytrain)\n",
    "        \n",
    "        ytrain_pred = clf.predict_proba(xtrain)[:,1]\n",
    "        yval_pred = clf.predict_proba(xval)[:,1]\n",
    "        ytest_pred = clf.predict_proba(X_test)[:,1]\n",
    "        \n",
    "        # filling blend data sets\n",
    "        blend_X[val_indices, 0] = yval_pred\n",
    "        blend_X_test_fold[:, fold_indice] = ytest_pred\n",
    "        \n",
    "        # evaluating model\n",
    "        train_error = log_loss(ytrain, ytrain_pred)\n",
    "        val_error = log_loss(yval, yval_pred)\n",
    "        l_train_error.append(train_error)\n",
    "        l_val_error.append(val_error)\n",
    "        print(\"train/val error: [{0:.4f}|{1:.4f}]\".format(train_error, val_error))\n",
    "        \n",
    "        \n",
    "    blend_X_test = np.mean(blend_X_test_fold, axis=1)\n",
    "    \n",
    "    diclogs = {'blend_X': blend_X,\n",
    "               'blend_Y': Y,\n",
    "               'blend_X_test': blend_X_test,\n",
    "               'test_idx': test_idx,\n",
    "               'clf_name': clf_name}\n",
    "        \n",
    "    #saving relevant information for blending later\n",
    "    filename = \"BLEND_{}_tr-val_{:.4f}-{:.4f}\".format(clf_name, np.mean(l_train_error), np.mean(l_val_error))\n",
    "    pickle.dump(diclogs, open(DATA_FOLDER+filename + \".p\" , 'wb'), protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    #Saving final predictions\n",
    "    output_filename = DATA_FOLDER + filename + '.csv'\n",
    "    np.savetxt(output_filename, np.vstack((test_idx, blend_X_test)).T,\n",
    "               delimiter=',', fmt='%i,%.10f', header='ID,PredictedProb', comments=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Average predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
